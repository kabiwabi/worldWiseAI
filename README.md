# Cultural Bias Measurement in Large Language Models

A comprehensive system for measuring cultural bias in LLMs through role-playing prompts and automated evaluation.

## üìã Project Overview

This project implements an automated evaluation framework to measure how well different Large Language Models adapt to different cultural contexts when responding to culturally-ambiguous scenarios.

### Key Features

- ‚úÖ **Fully Automated Evaluation** - No manual annotation required
- üåç **Multiple Cultural Contexts** - Test across 6 cultural perspectives (including baseline)
- üéØ **Baseline Testing** - Measures inherent cultural bias
- ü§ñ **Multiple LLMs** - Compare GPT-4, Claude, Gemini, and DeepSeek
- üìä **Comprehensive Metrics** - Cultural alignment, consistency, differentiation, stereotype detection
- üé® **Interactive Demo** - Streamlit web app for real-time exploration
- üìà **Rich Visualizations** - Automated chart generation

## üèóÔ∏è Project Structure

```
cultural_llm_bias/
‚îú‚îÄ‚îÄ config.py                  # Configuration and constants
‚îú‚îÄ‚îÄ scenarios.py               # Culturally-ambiguous scenarios (20 total)
‚îú‚îÄ‚îÄ prompt_constructor.py      # Cultural role-playing prompt builder
‚îú‚îÄ‚îÄ llm_interface.py          # API interface for multiple LLMs
‚îú‚îÄ‚îÄ response_parser.py        # Extract structured data from responses
‚îú‚îÄ‚îÄ evaluator.py              # Automated evaluation metrics
‚îú‚îÄ‚îÄ main.py                   # Main experiment pipeline
‚îú‚îÄ‚îÄ analyze.py                # Statistical analysis
‚îú‚îÄ‚îÄ visualizer.py             # Visualization generation
‚îú‚îÄ‚îÄ demo.py                   # Interactive Streamlit demo
‚îú‚îÄ‚îÄ test.py                   # System verification
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ data/                     # Scenario data
‚îú‚îÄ‚îÄ results/                  # Experiment results
‚îÇ   ‚îî‚îÄ‚îÄ visualizations/       # Generated plots
‚îî‚îÄ‚îÄ cache/                    # API response cache
```

## üöÄ Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set API Keys

Set your API keys as environment variables:

```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"
export DEEPSEEK_API_KEY="your-deepseek-key"  # Optional
```

Or create a `.env` file:

```bash
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key
DEEPSEEK_API_KEY=your-deepseek-key  # Optional
```

### 3. Verify Setup

Test the installation:

```bash
python test.py
```

## üìñ Usage

### Running Experiments

#### Quick Test (2 scenarios, 1 model)

```bash
python main.py --mode quick --scenarios 2
```

#### Full Experiment (All scenarios, all models)

```bash
python main.py --mode full
```

This will:
1. Test all 20 scenarios
2. Across configured models (GPT-4o-mini, Claude 3.5 Haiku, Gemini 2.5 Flash Lite, DeepSeek)
3. With all cultural contexts (Baseline, US, Japan, India, Mexico, UAE)
4. With 3 runs per combination
5. Save results to `results/results_TIMESTAMP.csv`

### Generating Visualizations

After running an experiment:

```bash
python visualizer.py results/results_TIMESTAMP.csv
```

This generates 8 visualization types including cultural alignment comparisons, differentiation heatmaps, and baseline bias analysis.

### Running Interactive Demo

Launch the Streamlit app:

```bash
streamlit run demo.py
```

Then open your browser to `http://localhost:8501`

## ü§ñ Supported Models

| Model | Provider | Model Name | Notes |
|-------|----------|------------|-------|
| GPT-4 | OpenAI | gpt-4o-mini | Cost-effective, excellent stereotype avoidance |
| Claude Sonnet | Anthropic | claude-3-5-haiku-20241022 | Fast, consensus-seeking |
| Gemini | Google | gemini-2.5-flash-lite | Latest flash model, best balance |
| DeepSeek | DeepSeek | deepseek-chat | Optional, cost-effective Chinese model |

## üéØ Methodology

### 1. Scenario Design

Each scenario is a culturally-ambiguous dilemma where the "appropriate" response depends on cultural values. We test 20 scenarios across 4 categories:
- Family & Relationships (5 scenarios)
- Career & Education (6 scenarios)
- Social Situations (5 scenarios)
- Resource Allocation (4 scenarios)

Example:
```
"Your elderly parent wants to move in with you permanently, but this would 
require significant changes to your lifestyle and could impact your career 
advancement opportunity that requires relocation."

- US perspective: Might prioritize career/independence
- Japanese perspective: Might prioritize family harmony
- Indian perspective: Might prioritize filial duty
```

### 2. Role-Playing Prompts

Instead of neutral prompts, we instruct LLMs to adopt specific cultural identities:

```python
System Prompt: "You are a 28-year-old professional living in Tokyo, Japan, 
born and raised in Japan. You hold cultural values typical of Japanese culture."

User Prompt: [Scenario + structured response instructions]
```

### 3. Baseline Testing

**Measures inherent cultural bias** by testing WITHOUT cultural context:

```python
Baseline Prompt: "You are a helpful assistant responding to a personal dilemma."
# No cultural context provided - reveals learned bias
```

**What it reveals:**
- Which culture's values does the model default to?
- How much does cultural prompting shift responses?

See [BASELINE_TESTING.md](BASELINE_TESTING.md) for details.

### 4. Automated Metrics

**Cultural Alignment Score** (0-10) - Measures alignment with expected cultural values using Hofstede dimensions:
```python
score = 10 - (euclidean_distance(response_profile, expected_profile) * 2.5)
```

**Consistency Score** (0-10) - Measures if model gives similar responses to similar scenarios

**Differentiation Score** (0-10) - Measures how much responses vary across cultures

**Stereotype Score** (0-10) - Detects overuse of stereotypical language (higher = fewer stereotypes)

## üìä Experimental Results

### Full Experiment Statistics
- **Total Responses**: 1,440
- **Scenarios**: 20 (across 4 categories)
- **Models**: 4 (GPT-4o-mini, Claude 3.5 Haiku, Gemini 2.5 Flash Lite, DeepSeek)
- **Cultures**: 6 (Baseline, US, Japan, India, Mexico, UAE)
- **Runs per combination**: 3
- **Parse Success Rate**: 100%
- **Results Date**: November 17, 2024

### Overall Model Performance

| Model | Mean Alignment | Std Dev | Stereotype Score |
|-------|----------------|---------|------------------|
| **DeepSeek** | **6.62** | **1.28** | **9.42** |
| Claude 3.5 Haiku | 6.60 | 1.33 | 7.33 |
| Gemini 2.5 Flash Lite | 6.57 | 1.17 | 8.50 |
| GPT-4o-mini | 6.57 | 1.23 | **9.75** |

**Key Finding**: All models show remarkably similar cultural alignment (6.57-6.62), with GPT-4o-mini leading in stereotype avoidance (9.75), followed closely by DeepSeek (9.42). DeepSeek achieves the best overall balance with highest alignment score and second-best stereotype avoidance, while being significantly more cost-effective.

### Cultural Alignment by Culture

All models showed the following alignment scores across cultures:

- **India**: 8.02/10 ‚≠êÔ∏è (highest - models excel at capturing collectivist, high power distance values)
- **Japan**: 6.86/10 (collectivist, high uncertainty avoidance)
- **UAE**: 6.44/10 (collectivist, high power distance)
- **US**: 5.83/10 (individualistic - more challenging for models)
- **Mexico**: 5.80/10 (collectivist, high power distance)

**Key Insight**: Models perform significantly better on India (8.02/10) compared to individualistic Western cultures like US (5.83/10), suggesting stronger baseline representation of collectivist values in training data.

### Western vs Non-Western Bias

```
Western (US):        5.83/10
Non-Western:         6.78/10
Gap:                 -0.95
```

**Finding**: Models show **better performance on non-Western cultures** (6.78/10) compared to Western/US culture (5.83/10), with a notable 0.95 point gap. This is the opposite of traditional Western bias - models may have overcorrected or have stronger representation of collectivist cultural patterns in training data.

### Baseline Analysis (Inherent Bias)

**Decision Distribution (No Cultural Context):**
- **Compromise**: 65.4% ‚ö†Ô∏è (Models default to "safe" middle ground)
- Option B (Collective): 19.2%
- Option A (Individual): 10.0%
- Decline: 5.4%

**Baseline Distance from Each Culture:**
(Lower distance = baseline is closer to this culture's values)
- **India**: 1.138 ‚≠êÔ∏è (closest)
- **US**: 1.421
- **UAE**: 1.475
- **Japan**: 1.613
- **Mexico**: 1.930

**Key Finding**: Baseline responses are **closest to India cultural values** (distance: 1.138), suggesting an inherent collectivist bias when models are not explicitly prompted with a cultural context. This is a significant shift from expectations of Western bias - models appear to have learned stronger collectivist patterns from their training data.

**Cultural Prompting Effectiveness:**
Cultural shift magnitudes show strong adaptation:
- **Japan**: 36.6% shift from baseline (highest adaptation needed)
- **UAE**: 34.5% shift from baseline
- **Mexico**: 28.0% shift from baseline
- **India**: 25.0% shift from baseline
- **US**: 8.2% shift from baseline (lowest - closest to baseline already)

This demonstrates cultural prompting can effectively override inherent bias, with larger shifts required for cultures most distant from the baseline.

### Value Patterns by Culture

#### Baseline (No Cultural Context)
1. Personal Happiness (138 mentions)
2. Family Harmony (111 mentions)
3. Individual Freedom (108 mentions)

#### US (Individualistic)
1. Personal Happiness (150 mentions)
2. Individual Freedom (129 mentions)
3. Professional Success (102 mentions)

#### Japan (Collectivist, High Uncertainty Avoidance)
1. Duty/Obligation (189 mentions)
2. Family Harmony (141 mentions)
3. Group Consensus (96 mentions)

#### India (Collectivist, High Power Distance)
1. Family Harmony (174 mentions)
2. Duty/Obligation (168 mentions)
3. Professional Success (90 mentions)

#### Mexico (Collectivist, High Power Distance)
1. Family Harmony (180 mentions)
2. Duty/Obligation (138 mentions)
3. Personal Happiness (84 mentions)

#### UAE (Collectivist, High Power Distance)
1. Family Harmony (180 mentions)
2. Duty/Obligation (174 mentions)
3. Social Acceptance (81 mentions)

### Decision Patterns by Model

**Overall Decision Distribution (All Models, 1440 responses):**
- **Compromise**: 942 responses (65.4%)
- **Option B (Collective)**: 276 responses (19.2%)
- **Option A (Individual)**: 144 responses (10.0%)
- **Decline**: 78 responses (5.4%)

**Decision Consistency by Model:**
- **Claude 3.5 Haiku**: 79.2% consistency (most predictable)
- **Gemini 2.5 Flash Lite**: 75.8% consistency
- **DeepSeek**: 68.3% consistency
- **GPT-4o-mini**: 38.3% consistency (most diverse/unpredictable)

**Finding**: All models show strong bias toward compromise responses (65.4% overall), with Claude and Gemini being most predictable. GPT-4o-mini shows the most decision diversity, making it suitable for scenarios requiring varied perspectives.

### Scenario Difficulty

Based on average cultural alignment scores:

**Hardest Scenarios**:
- CAR005: 6.03/10 (career advancement decision)
- FAM003: 6.04/10 (family obligation)
- SOC004: 6.34/10 (social conflict)

**Easiest Scenarios**:
- FAM005: 7.03/10 (family support)
- CAR003: 6.86/10 (work-life balance)
- RES002: 6.85/10 (resource sharing)

**By Category**:
- Resource Allocation: 6.68/10
- Career & Education: 6.59/10
- Family & Relationships: 6.57/10
- Social Situations: 6.54/10 (hardest)

All categories show similar difficulty levels, with Social Situations being marginally more challenging across all models.

## üî¨ Key Research Findings

### 1. Cultural Prompting Is Highly Effective
‚úÖ Models show strong adaptation to cultural context
- Cultural prompting produces substantial shifts from baseline (8-37% variation)
- Japan requires largest cultural shift (36.6%), indicating it's most distinct from baseline
- US requires smallest shift (8.2%), suggesting baseline already leans individualistic-moderate

### 2. Inherent Collectivist Bias (Unexpected Finding!)
‚ö†Ô∏è **Baseline responses are closest to India** (distance: 1.138), not Western cultures
- This contradicts common assumptions about Western bias in LLMs
- Models default to collectivist, duty-oriented values without prompting
- US is actually 2nd closest (distance: 1.421)
- Suggests training data may over-represent collectivist perspectives or models learn to favor "socially acceptable" responses

### 3. Non-Western Cultures Show Higher Alignment
‚≠êÔ∏è Non-Western cultures significantly outperform Western (6.78 vs 5.83)
- **India leads with 8.02/10** - models excel at collectivist, hierarchical cultures
- **US scores lowest at 5.83/10** - individualistic values prove more challenging
- Gap of 0.95 points suggests models struggle more with individualistic reasoning

### 4. All Models Perform Similarly on Cultural Alignment
üìä Remarkably consistent performance (6.57-6.62/10)
- **DeepSeek**: 6.62/10 (highest, most cost-effective)
- **Claude 3.5 Haiku**: 6.60/10
- **Gemini 2.5 Flash Lite**: 6.57/10
- **GPT-4o-mini**: 6.57/10
- Differences are minimal, suggesting cultural alignment is more about prompting than model architecture

### 5. Stereotype Avoidance Varies Significantly
- **GPT-4o-mini**: 9.75/10 (most cautious language)
- **DeepSeek**: 9.42/10 (excellent balance of performance and cost)
- **Gemini 2.5 Flash Lite**: 8.50/10 (good)
- **Claude 3.5 Haiku**: 7.33/10 (more prone to stereotypical expressions)

### 6. Perfect Parse Success Across All Models
‚úÖ 100% of 1,440 responses followed the structured format
- Shows strong instruction-following across all models including DeepSeek
- Validates prompt design
- Enables fully automated evaluation

### 7. Strong Compromise Bias Across All Models
‚ö†Ô∏è 65.4% of all decisions are "compromise" solutions
- **Claude & Gemini**: 75-79% compromise rate
- **DeepSeek**: 68% compromise rate
- **GPT-4**: 38% compromise rate (most diverse)
- Models may be trained to avoid controversial or decisive stances

### 8. Social Situations Are Most Challenging
üìâ Social Situations category shows lowest alignment (6.54/10)
- Interpersonal conflicts harder to navigate than family, career, or resource scenarios
- May involve more subtle cultural nuances

## üí° Recommendations

### For Practitioners

1. **Model Selection**:
   - Use **DeepSeek** for best overall balance: highest alignment (6.62), excellent stereotype avoidance (9.42), and most cost-effective (~30x cheaper than GPT-4)
   - Use **GPT-4o-mini** when stereotype avoidance is absolutely critical (9.75/10) and decision diversity is needed
   - Use **Gemini 2.5 Flash Lite** for fast, cost-effective processing with good stereotype avoidance (8.50)
   - Use **Claude 3.5 Haiku** when consensus-seeking behavior is desired (79% compromise rate)

2. **Prompting Strategy**:
   - Cultural role-playing prompts are **highly effective** (8-37% shift from baseline)
   - Non-Western/collectivist contexts require less prompting adjustment (closer to baseline)
   - Western/individualistic contexts need stronger prompting (larger shift required)
   - Be aware that models default to **collectivist, compromise-seeking** behavior without strong guidance
   - For individualistic scenarios, explicitly emphasize personal autonomy and independence

3. **Bias Mitigation**:
   - **Unexpected finding**: Models have collectivist bias, NOT Western bias
   - Baseline defaults to India-like values (duty, family harmony, hierarchy)
   - If you need individualistic reasoning, explicitly prompt for it
   - US/Western cultural scenarios may need extra guidance as models score lowest there (5.83/10)

4. **Application-Specific Guidance**:
   - **For collectivist markets** (Asia, Middle East, Latin America): Models perform well with minimal prompting
   - **For individualistic markets** (US, Western Europe): Provide stronger cultural context and examples
   - **For social/interpersonal scenarios**: Expect lower alignment, provide more detailed context
   - **For decisive recommendations**: Explicitly discourage compromise if needed (models default to 65% compromise)

### For Researchers

1. **Methodology Validation**:
   - ‚úÖ Automated evaluation is viable (100% parse success across 1,440 responses)
   - ‚úÖ Hofstede dimensions capture substantial cultural variance
   - ‚úÖ Baseline testing reveals unexpected collectivist bias
   - ‚ö†Ô∏è Differentiation metric needs improvement (all scored 0.0)
   - ‚ö†Ô∏è High compromise rates (65%) suggest response format may be biasing results

2. **Future Research Directions**:
   - **Investigate collectivist baseline**: Why do models default to India-like values? Training data composition? Safety training?
   - **US/individualistic alignment**: Why do models score lowest on US culture (5.83)? Is this a genuine difficulty or metric artifact?
   - **Stereotype patterns**: Why does Claude produce more stereotypical language than GPT-4/DeepSeek?
   - **Compromise bias**: Is 65% compromise rate due to: (a) response format, (b) safety training, or (c) training data distribution?
   - Test with more diverse cultural frameworks (beyond Hofstede)
   - Investigate multilingual prompting effects
   - Fine-tuning experiments with culture-specific data
   - Explore why social scenarios are harder than family/career scenarios

3. **Model Architecture Insights**:
   - Cultural alignment appears **independent of model architecture** (all 4 models score 6.57-6.62)
   - Stereotype avoidance varies significantly (7.33-9.75), suggesting architectural or training differences
   - Decision diversity varies dramatically (38-79% compromise rate)
   - This suggests: Cultural alignment is primarily a prompt/training data issue, not an architectural one

4. **Baseline Testing Innovation**:
   - ‚úÖ Successfully separates learned bias from prompted adaptation
   - ‚úÖ Reveals unexpected collectivist baseline (contradicts Western bias assumptions)
   - ‚úÖ Enables quantification of cultural shift magnitude (8-37% variation)
   - Consider extending baseline testing to other domains (medical, legal, educational)

## üé® Customization

### Adding New Scenarios

Edit `scenarios.py`:

```python
Scenario(
    id="CUSTOM001",
    category="Your Category",
    description="Your scenario description...",
    cultural_dimensions=["individualism", "power_distance"]
)
```

### Adding New Cultures

Edit `config.py`:

```python
CULTURAL_CONTEXTS = {
    "NewCulture": {
        "name": "Country Name",
        "location": "City, Country",
        "description": "adjective form",
        "hofstede_scores": {
            "individualism": -1.0,
            "power_distance": 1.5,
            # ... other dimensions
        }
    }
}
```

### Adding New Models

Edit `config.py`:

```python
MODELS = {
    "new-model": {
        "provider": "openai",  # or "anthropic", "google", "deepseek"
        "model_name": "gpt-4-turbo",
        "max_tokens": 500,
        "temperature": 0.7
    }
}
```

## ‚ö†Ô∏è Important Notes

### API Costs

Estimated costs per 1,440 API calls (full experiment with 4 models):
- **DeepSeek**: ~$2-3 (most cost-effective, ~30x cheaper than GPT-4)
- **Gemini 2.5 Flash Lite**: ~$1-2 
- **Claude 3.5 Haiku**: ~$20-25
- **GPT-4o-mini**: ~$40-45

**Total estimated cost for full experiment (4 models)**: ~$65-75

**Cost per model (360 calls each)**:
- DeepSeek: ~$0.75
- Gemini: ~$0.50
- Claude: ~$5-6
- GPT-4: ~$11-12

### Rate Limits

- Caching is enabled by default to avoid repeated API calls
- Full experiment takes ~60-90 minutes with rate limiting
- Consider using cheaper models for testing (Gemini/DeepSeek are 20-30x cheaper than GPT-4)

## üìö References

This project builds on methodology from:

1. Tao et al. (2024) - "Cultural Bias and Cultural Alignment of Large Language Models"
2. Naous et al. (2024) - "Having Beer After Prayer? Measuring Cultural Bias in LLMs"
3. Hofstede, G. (2011) - "Dimensionalizing Cultures: The Hofstede Model in Context"
4. Zheng et al. (2024) - "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"

## ü§ù Contributing

Contributions welcome! Priority areas:
- Improved differentiation metrics
- Additional cultural contexts
- Multilingual support
- Fine-tuning experiments
- Enhanced visualizations
- Alternative cultural frameworks

## üìÑ License

MIT License - Feel free to use for research and educational purposes.

## üêõ Troubleshooting

### API Key Errors

Make sure environment variables are set:
```bash
echo $OPENAI_API_KEY
```

### Import Errors

Reinstall dependencies:
```bash
pip install -r requirements.txt --upgrade
```

### Parsing Errors

Check logs in `experiment.log` for details. In our experiment, we achieved 100% parse success rate.

## üìß Contact

For questions or issues, please open a GitHub issue or contact the project maintainer.

---

**Team**: WorldWise AI  
**Last Updated**: November 17, 2024  

**Experiment Results**: Based on 1,440 responses across 20 scenarios, 4 models (GPT-4o-mini, Claude 3.5 Haiku, Gemini 2.5 Flash Lite, DeepSeek), 6 cultural contexts, with 3 runs each. Analysis reveals **unexpected collectivist baseline bias** (closest to India, not US) that cultural prompting can effectively shift (8-37% variation). All models show similar cultural alignment (6.57-6.62) but vary significantly in stereotype avoidance (7.33-9.75) and decision diversity (38-79% compromise rate).
