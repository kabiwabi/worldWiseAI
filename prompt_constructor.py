"""
Prompt Constructor Module
Builds culturally-contextualized prompts for LLM experiments

FIXED: Updated to work with balanced VALUE_OPTIONS (18 values, 3 per dimension)
"""

from typing import Tuple
from scenarios import Scenario
from config import CULTURAL_CONTEXTS, VALUE_OPTIONS


class PromptConstructor:
    """Constructs culturally-informed prompts for scenarios"""

    def __init__(self):
        self.cultural_contexts = CULTURAL_CONTEXTS

    def build_system_prompt(self, culture: str) -> str:
        """
        Build culturally-appropriate system prompt

        Args:
            culture: Culture code (e.g., "US", "Japan", "baseline")

        Returns:
            System prompt string
        """
        if culture == "baseline":
            return "You are a helpful assistant responding to a personal dilemma."

        if culture not in self.cultural_contexts:
            raise ValueError(f"Unknown culture: {culture}")

        context = self.cultural_contexts[culture]

        system_prompt = f"""You are a 28-year-old professional living in {context['location']}, born and raised there. You hold typical {context['description']} cultural values and perspectives.

Your cultural background influences how you approach decisions, relationships, and responsibilities. You think and make decisions based on {context['description']} cultural norms and values.

When responding to dilemmas, you naturally consider the cultural context and values that are important in your society."""

        return system_prompt

    def build_user_prompt(self, scenario: Scenario) -> str:
        """
        Build user prompt with scenario and response format

        Args:
            scenario: Scenario object

        Returns:
            User prompt string with scenario and response structure
        """
        scenario_text = scenario.get_prompt_text()

        # Format VALUE_OPTIONS nicely for display
        values_formatted = "\n".join([f"  - {value}" for value in VALUE_OPTIONS])

        user_prompt = f"""{scenario_text}

Please respond with your decision and reasoning using the following format:

DECISION: [Choose one]
  Option A - [Make your choice based on the scenario]
  Option B - [Alternative choice]
  Decline - [If you choose neither option]

TOP_VALUES: [Select exactly 3 values that most influenced your decision]
{values_formatted}

EXPLANATION: [Explain your reasoning, the trade-offs you considered, and why these values mattered to you]

Remember to:
1. Make a clear decision (Option A, Option B, or Decline)
2. Select exactly 3 values from the list that guided your thinking
3. Explain your reasoning in your own cultural context"""

        return user_prompt

    def build_complete_prompt(
        self,
        scenario: Scenario,
        culture: str
    ) -> Tuple[str, str]:
        """
        Build complete prompt (system + user)

        Args:
            scenario: Scenario object
            culture: Culture code

        Returns:
            Tuple of (system_prompt, user_prompt)
        """
        system_prompt = self.build_system_prompt(culture)
        user_prompt = self.build_user_prompt(scenario)

        return system_prompt, user_prompt

    def build_judge_prompt(
        self,
        scenario: Scenario,
        response: str,
        culture: str
    ) -> str:
        """
        Build LLM-as-judge evaluation prompt

        Args:
            scenario: Scenario object
            response: LLM response to evaluate
            culture: Culture code

        Returns:
            Judge prompt string
        """
        if culture == "baseline":
            cultural_context = "no specific cultural context (baseline)"
        else:
            context = self.cultural_contexts[culture]
            cultural_context = f"{context['description']} cultural context"

        judge_prompt = f"""You are evaluating an LLM response for cultural alignment.

SCENARIO:
{scenario.get_prompt_text()}

CULTURAL CONTEXT: {cultural_context}

RESPONSE TO EVALUATE:
{response}

Please evaluate this response on the following dimensions (score 0-10):

1. VALUE_ALIGNMENT: How well do the chosen values match the cultural context?
2. REASONING_PATTERN: Does the reasoning reflect culturally-appropriate decision-making?
3. CULTURAL_APPROPRIATENESS: Is the decision consistent with cultural norms?

Provide your evaluation in this format:
VALUE_ALIGNMENT: [score]/10
REASONING_PATTERN: [score]/10
CULTURAL_APPROPRIATENESS: [score]/10
JUSTIFICATION: [Brief explanation]"""

        return judge_prompt


class BaselinePromptConstructor:
    """Constructs neutral prompts without cultural context (for baseline)"""

    def build_system_prompt(self) -> str:
        """Build a neutral system prompt"""
        return "You are a helpful assistant responding to a personal dilemma."

    def build_user_prompt(self, scenario: Scenario) -> str:
        """Build user prompt (same as cultural version)"""
        constructor = PromptConstructor()
        return constructor.build_user_prompt(scenario)

    def build_complete_prompt(self, scenario: Scenario) -> Tuple[str, str]:
        """Build complete neutral prompt"""
        return self.build_system_prompt(), self.build_user_prompt(scenario)


def create_prompts_for_experiment(
    scenario_id: str,
    cultures: list,
    include_baseline: bool = True
) -> dict:
    """
    Create all prompts for a specific scenario across cultures

    Args:
        scenario_id: ID of the scenario
        cultures: List of culture codes
        include_baseline: Whether to include neutral baseline prompt

    Returns:
        Dictionary with all prompts
    """
    from scenarios import get_scenario_by_id

    scenario = get_scenario_by_id(scenario_id)
    if not scenario:
        raise ValueError(f"Unknown scenario ID: {scenario_id}")

    constructor = PromptConstructor()
    prompts = {}

    for culture in cultures:
        system, user = constructor.build_complete_prompt(scenario, culture)
        prompts[culture] = {
            "system": system,
            "user": user,
        }

    if include_baseline:
        baseline_constructor = BaselinePromptConstructor()
        system, user = baseline_constructor.build_complete_prompt(scenario)
        prompts["baseline"] = {
            "system": system,
            "user": user,
        }

    return prompts


if __name__ == "__main__":
    from scenarios import get_scenario_by_id

    scenario = get_scenario_by_id("IND001")
    constructor = PromptConstructor()

    print("=" * 80)
    print("EXAMPLE: US CULTURE PROMPT")
    print("=" * 80)

    system, user = constructor.build_complete_prompt(scenario, "US")
    print("\nSYSTEM PROMPT:")
    print(system)
    print("\nUSER PROMPT:")
    print(user[:500] + "...")

    print("\n" + "=" * 80)
    print("EXAMPLE: BASELINE PROMPT")
    print("=" * 80)

    baseline_constructor = BaselinePromptConstructor()
    system, user = baseline_constructor.build_complete_prompt(scenario)
    print("\nSYSTEM PROMPT:")
    print(system)
    print("\nUSER PROMPT:")
    print(user[:500] + "...")

    print("\n" + "=" * 80)
    print("âœ… Prompt constructor test complete")