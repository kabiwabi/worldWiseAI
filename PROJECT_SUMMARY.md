# Project Summary: Cultural Bias Measurement in LLMs

## ðŸ“‹ Complete File Index & System Overview

**Version**: 2.0  
**Last Updated**: November 17, 2025  
**Status**: Production Ready âœ…

---

## ðŸŽ¯ Project Overview

Automated framework for measuring cultural bias in Large Language Models through:
- **Baseline testing** - Reveals inherent cultural bias without prompting
- **Cultural prompting** - Tests adaptation to specific cultural contexts
- **Automated metrics** - 4 evaluation dimensions across 6 cultures
- **Statistical analysis** - ANOVA, t-tests, effect sizes
- **Rich visualizations** - 11 automated chart types

**Key Finding**: Models exhibit inherent **India cultural bias** (distance: 1.066) in baseline testing.

---

## ðŸ“ Core System Files (6 files)

### 1. `config.py` (5.4 KB)

**Purpose**: Central configuration for entire system

**Contents**:
- API keys (OpenAI, Anthropic, Google, DeepSeek)
- Model configurations (4 models: GPT-4o-mini, Claude, Gemini, DeepSeek)
- Cultural contexts (6: baseline + 5 cultures)
- Hofstede dimension scores (official research data)
- System constants (paths, evaluation settings)

**Key Data Structures**:
```python
MODELS = {
    "gpt-4": {...},
    "claude-sonnet": {...},
    "gemini": {...},
    "deepseek": {...}
}

CULTURAL_CONTEXTS = {
    "baseline": {...},  # NEW: No cultural context
    "US": {...},
    "Japan": {...},
    "India": {...},
    "Mexico": {...},
    "UAE": {...}
}
```

**When to Edit**:
- Adding new models
- Adding new cultures
- Adjusting Hofstede scores
- Changing system paths

---

### 2. `scenarios.py` (11 KB)

**Purpose**: Defines all culturally-ambiguous scenarios

**Organization**: 4 categories, 20 total scenarios
- **Family & Relationships** (5): FAM001-FAM005
- **Career & Education** (6): CAR001-CAR006
- **Social Situations** (5): SOC001-SOC005
- **Resource Allocation** (4): RES001-RES004

**Scenario Structure**:
```python
@dataclass
class Scenario:
    id: str                    # e.g., "FAM001"
    category: str              # e.g., "Family & Relationships"
    title: str                 # Short title
    scenario: str              # Scenario description
    option_a: str              # First choice
    option_b: str              # Second choice
    relevant_dimensions: List  # Hofstede dimensions
```

**Example Scenario**:
```python
Scenario(
    id="FAM001",
    category="Family & Relationships",
    title="Elderly Parent Care",
    scenario="Your elderly parent needs full-time care...",
    option_a="Move parent to assisted living",
    option_b="Quit job to care for parent at home",
    relevant_dimensions=["individualism", "long_term_orientation"]
)
```

**When to Edit**:
- Adding custom scenarios
- Testing domain-specific situations
- Adjusting scenario difficulty

---

### 3. `prompt_constructor.py` (7.2 KB)

**Purpose**: Builds culturally-contextualized prompts

**Key Classes**:

**`PromptConstructor`** - Main prompt builder
- Creates system prompts with cultural identity
- Creates user prompts with response structure
- Detects baseline and uses neutral prompts

**`BaselinePromptConstructor`** - Baseline-specific
- Generates neutral prompts (no cultural context)
- Used for baseline bias detection

**Prompt Structure**:
```python
# Cultural Prompt
System: "You are a 28-year-old professional in Tokyo, Japan,
         born and raised in Japan. You hold typical Japanese
         cultural values including harmony, duty, group consensus..."

User: "[Scenario]
       Respond with: DECISION: [A/B/Decline]
                     TOP VALUES: [value1, value2, value3]
                     EXPLANATION: [reasoning]"

# Baseline Prompt
System: "You are a helpful assistant responding to a personal dilemma."

User: "[Same scenario with same response structure]"
```

**When to Edit**:
- Adjusting cultural framing strength
- Changing response structure
- Adding new prompt templates

---

### 4. `llm_interface.py` (12 KB)

**Purpose**: Handles all API calls with caching

**Supported Providers**:
- **OpenAI** (GPT-4o-mini)
- **Anthropic** (Claude 3.5 Haiku)
- **Google** (Gemini 2.5 Flash)
- **DeepSeek** (DeepSeek-chat)

**Key Features**:
- Automatic caching (avoids duplicate API calls)
- Rate limiting and error handling
- Unified interface for all providers
- Retry logic with exponential backoff

**Cache System**:
```python
cache_key = hash(model + system_prompt + user_prompt + temperature)
if cache_key in cache:
    return cached_response  # Instant, no API call
else:
    response = call_api()
    save_to_cache(cache_key, response)
    return response
```

**When to Edit**:
- Adding new LLM providers
- Adjusting retry logic
- Changing cache behavior

---

### 5. `response_parser.py` (10 KB)

**Purpose**: Extracts structured data from LLM responses

**Key Classes**:

**`ParsedResponse`** - Parsed response dataclass
```python
@dataclass
class ParsedResponse:
    raw_text: str           # Full response
    decision: str           # "Option A", "Option B", "Decline"
    top_values: List[str]   # ["Family Harmony", "Duty", ...]
    explanation: str        # Reasoning text
    parse_success: bool     # True if parsing succeeded
    parse_errors: List      # Any parsing issues
```

**`ResponseParser`** - Parser class
- Handles multiple response formats
- Extracts decision with multiple patterns
- Extracts values from various formats
- Robust error handling

**Parsing Patterns**:
```python
# Accepted decision formats:
"DECISION: Option A"
"Decision: A"
"I choose option A"
"Option A"

# Accepted value formats:
"TOP VALUES: Family Harmony, Duty, Respect"
"Values: [Family Harmony, Duty]"
"I value family harmony, duty, and respect"
```

**Parse Success Rate**: 100% in current tests

**When to Edit**:
- Adding new response formats
- Adjusting value extraction
- Handling new LLM output patterns

---

### 6. `evaluator.py` (13 KB)

**Purpose**: Computes all automated evaluation metrics

**Key Metrics**:

**1. Cultural Alignment (0-10)**
```python
distance = euclidean(model_values, expected_values)
alignment = 10 - distance
```

**2. Consistency (0-10)**
- Measures response stability across similar scenarios
- Based on decision and value consistency

**3. Differentiation (0-10)**
- Measures response variation across cultures
- High = good cultural adaptation
- Low = same responses regardless of culture

**4. Stereotype Score (0-10)**
- Detects stereotypical language patterns
- Keywords: "always", "typical", "traditional"
- Higher = less stereotyping

**New Functionality**:

**`calculate_baseline_bias()`** - Baseline bias detection
```python
def calculate_baseline_bias(
    baseline_responses,
    cultural_contexts,
    scenario_dimensions
) -> Dict[str, float]:
    """
    1. Infer cultural profile from baseline responses
    2. Calculate distance to each culture
    3. Return distances (lower = closer match)
    """
```

**When to Edit**:
- Adding new metrics
- Adjusting evaluation formulas
- Changing stereotype detection

---

## ðŸš€ Execution Files (5 files)

### 7. `main.py` (9.2 KB)

**Purpose**: Main experiment orchestration

**Key Class**: `ExperimentRunner`

**Features**:
- Progress tracking with tqdm
- Error logging and recovery
- Automatic result saving (CSV + JSON)
- Summary statistics generation
- Baseline bias calculation

**Usage**:
```bash
# Quick test (2 scenarios, baseline + US + Japan)
python main.py --mode quick --scenarios 2

# Full experiment (20 scenarios, all 6 cultures)
python main.py --mode full

# Custom configuration
python main.py --mode custom --scenarios 5 --models "gpt-4,deepseek"
```

**Output Files**:
- `results_TIMESTAMP.csv` - All raw responses and metrics
- `results_TIMESTAMP.json` - Structured JSON version
- `summary_TIMESTAMP.json` - Aggregated statistics + baseline bias
- `experiment.log` - Detailed execution log

**Baseline Testing**:
- Automatically included in all experiments
- Can be disabled with `include_baseline=False`
- Baseline always runs first for bias detection

---

### 8. `demo.py` (9.5 KB)

**Purpose**: Interactive Streamlit web application

**Features**:
- Real-time scenario testing
- Side-by-side model comparison (1-4 models)
- Culture selection (1-6 cultures, including baseline)
- Interactive visualizations
- Export results to CSV
- Metric explanations and tooltips

**UI Components**:
- **Sidebar**: Scenario and configuration selection
- **Response Tab**: Side-by-side response comparison
- **Analysis Tab**: Metric visualizations
- **Metrics Tab**: Detailed metric breakdowns

**Launch**:
```bash
streamlit run demo.py
# Opens at http://localhost:8501
```

**Use Cases**:
- Quick scenario testing
- Model comparison research
- Baseline bias exploration
- Teaching and demonstrations

---

### 9. `visualizer.py` (11 KB)

**Purpose**: Generates all visualization charts

**11 Visualization Types**:

1. **Cultural Alignment by Model** - Bar chart comparison
2. **Differentiation Heatmap** - Culture Ã— Model heatmap
3. **Decision Distribution** - Option A/B/Decline breakdown
4. **Value Frequency** - Most common values by culture
5. **Stereotype Scores** - Model comparison
6. **Model Comparison Radar** - Multi-metric spider chart
7. **Category Performance** - Alignment by scenario category
8. **Baseline Comparison** - Baseline vs cultural prompting
9. **Cultural Shift Magnitude** - Adaptation strength chart
10. **Scenario Difficulty** - Hardest and easiest scenarios
11. **Decision Patterns by Model** - Stacked bar chart

**Usage**:
```bash
python visualizer.py results/results_TIMESTAMP.csv
```

**Output**: `results/visualizations/*.png` (11 files)

**Configuration**:
- Customizable figure size in `config.py`
- DPI: 300 (publication quality)
- Style: seaborn with custom color palettes

---

### 10. `analyze.py` (8.2 KB)

**Purpose**: Comprehensive statistical analysis

**Analysis Components**:

**1. Baseline Bias Detection**
- Calculates distance from baseline to each culture
- Identifies closest cultural match
- Interprets inherent bias

**2. Model Performance Comparison**
- ANOVA for overall significance
- Pairwise t-tests with Bonferroni correction
- Cohen's d effect sizes

**3. Cultural Differences Analysis**
- ANOVA across cultures
- Mean alignment by culture
- Standard deviations

**4. Consistency Analysis**
- Model consistency across runs
- Culture consistency
- Scenario-level variation

**5. Decision Pattern Analysis**
- Decision distribution by culture
- Decision distribution by model
- Decision entropy (diversity)

**6. Value Frequency Analysis**
- Top values by culture
- Value shifts (baseline vs prompted)

**7. Category Performance**
- Alignment by scenario category
- Hardest and easiest scenarios

**Usage**:
```bash
python analyze.py results/results_TIMESTAMP.csv
```

**Output**:
- `results/analysis_report_TIMESTAMP.txt` (comprehensive report)
- Console output with formatted tables

---

### 11. `test.py` (7.7 KB)

**Purpose**: System verification and diagnostics

**Test Categories**:

1. **Configuration Tests**
   - Config loading
   - API key detection
   - Path verification

2. **Scenario Tests**
   - Scenario loading (20 scenarios)
   - Scenario structure validation

3. **Prompt Construction Tests**
   - Baseline prompts (neutral)
   - Cultural prompts (with context)

4. **Response Parsing Tests**
   - Multiple format handling
   - Error recovery

5. **Evaluation Tests**
   - Metric calculations
   - Baseline handling

6. **LLM Interface Tests**
   - API client initialization
   - Cache functionality

**Usage**:
```bash
python test.py
```

**Expected Output**:
```
ðŸ§ª Running Cultural LLM Bias Framework Tests

Testing Core Components...
âœ… PASS: Configuration loading
âœ… PASS: Scenario definitions
âœ… PASS: Prompt construction
âœ… PASS: Response parsing
âœ… PASS: Evaluation metrics
âœ… PASS: LLM interface

Testing API Keys...
âœ… OpenAI API key configured
âœ… Anthropic API key configured
âœ… Google API key configured
âœ… DeepSeek API key configured

All tests passed! âœ¨
```

---

## ðŸ“š Documentation Files (4 files)

### 12. `README.md` (40 KB)

**Purpose**: Complete project documentation

**Sections**:
- Project overview and features
- Quick start guide
- Experimental results (7 key findings)
- Methodology explanation
- Statistical analysis
- Research implications
- Limitations and future work
- Model comparison and costs
- Advanced usage examples

**Target Audience**: Researchers, practitioners, new users

---

### 13. `QUICKSTART.md` (15 KB)

**Purpose**: 5-minute setup and getting started

**Sections**:
- Installation (5 steps)
- Quick test workflow
- Visualization generation
- Troubleshooting
- Example workflows
- Metric interpretation guide

**Target Audience**: New users wanting rapid onboarding

---

### 14. `BASELINE_TESTING.md` (8 KB)

**Purpose**: Detailed baseline testing methodology

**Sections**:
- What is baseline testing
- Why it matters for cultural bias
- How to use baseline feature
- Results interpretation
- Academic context and citations
- Technical implementation details

**Target Audience**: Researchers, methodology reviewers

---

### 15. `PROJECT_SUMMARY.md` (This file)

**Purpose**: Complete file index and code overview

**Sections**:
- File-by-file descriptions
- Code statistics
- Workflow examples
- Output file reference
- Key features summary

**Target Audience**: Developers, code reviewers

---

## ðŸ“ˆ Output Files (Auto-generated)

### Experiment Results

**`results_TIMESTAMP.csv`** (raw data)
- All responses and metrics
- Columns: scenario_id, model, culture, decision, top_values, explanation, alignment, consistency, differentiation, stereotype_score, parse_success

**`results_TIMESTAMP.json`** (structured)
- Same data in JSON format
- Nested structure by model â†’ culture â†’ scenario

**`summary_TIMESTAMP.json`** (statistics)
- Mean, std, min, max by model
- Mean, std by culture
- **NEW**: Baseline bias detection results
- Overall statistics

**`experiment.log`** (execution log)
- Timestamped execution trace
- API calls logged
- Errors and warnings
- Progress milestones

### Analysis Reports

**`analysis_report_TIMESTAMP.txt`**
- Baseline bias detection
- Model comparison (ANOVA)
- Cultural differences (ANOVA)
- Consistency analysis
- Decision patterns
- Value frequency
- Category performance

### Visualizations (11 files)

All saved to `results/visualizations/`:
1. `cultural_alignment_by_model.png`
2. `differentiation_heatmap.png`
3. `decision_distribution.png`
4. `value_frequency.png`
5. `stereotype_scores.png`
6. `model_comparison_radar.png`
7. `category_performance.png`
8. `baseline_comparison.png`
9. `cultural_shift_magnitude.png`
10. `scenario_difficulty.png`
11. `decision_patterns_by_model.png`

---

## ðŸ’¡ Code Statistics

### Lines of Code
- **Core System**: ~3,000 lines (6 files)
- **Execution**: ~2,500 lines (5 files)
- **Documentation**: ~8,000 lines (4 files)
- **Total**: ~13,500 lines

### Test Coverage
- 6 test categories in `test.py`
- 100% parse success rate
- All core components tested

### Configuration
- **Scenarios**: 20 predefined, easily extensible
- **Cultures**: 6 (baseline + 5 cultures)
- **Models**: 4 (GPT-4o-mini, Claude, Gemini, DeepSeek)
- **Metrics**: 4 automated dimensions

---

## ðŸ”„ Typical Workflows

### Workflow 1: Quick Test

```bash
# 1. Install
pip install -r requirements.txt
export OPENAI_API_KEY="..."

# 2. Verify
python test.py

# 3. Quick test
python main.py --mode quick --scenarios 2

# 4. Visualize
python visualizer.py results/results_*.csv

# 5. Analyze
python analyze.py results/results_*.csv
```

**Time**: 5 minutes  
**Cost**: $0.10

---

### Workflow 2: Model Comparison Research

```bash
# 1. Test all models
python main.py --mode full

# 2. Generate visualizations
python visualizer.py results/results_*.csv

# 3. Statistical analysis
python analyze.py results/results_*.csv > analysis.txt

# 4. Review model rankings
cat results/summary_*.json | grep -A 20 "by_model"

# 5. Check stereotype scores
open results/visualizations/stereotype_scores.png
```

**Time**: 45 minutes  
**Cost**: $20-40

---

### Workflow 3: Baseline Bias Investigation

```bash
# 1. Run with baseline
python main.py --mode quick --scenarios 5

# 2. Extract baseline bias
cat results/summary_*.json | grep -A 15 "baseline_bias"

# 3. Analyze cultural shift
python analyze.py results/results_*.csv | grep -A 25 "BASELINE BIAS"

# 4. Visualize shift magnitude
open results/visualizations/cultural_shift_magnitude.png
```

**Time**: 10 minutes  
**Cost**: $0.25

---

### Workflow 4: Custom Scenario Testing

```bash
# 1. Edit scenarios.py (add custom scenarios)
nano scenarios.py

# 2. Test new scenarios
python main.py --mode custom --scenarios 3

# 3. Interactive exploration
streamlit run demo.py

# 4. Review results
cat results/summary_*.json
```

**Time**: 30 minutes  
**Cost**: $5-10

---

## ðŸŽ¯ Key Features Summary

âœ… **Baseline Testing** - Detects inherent cultural bias without prompting  
âœ… **Multi-Model** - Supports 4 major LLM providers  
âœ… **Multi-Cultural** - Tests 6 cultural contexts (baseline + 5)  
âœ… **Automated Metrics** - 4 evaluation dimensions  
âœ… **Statistical Rigor** - ANOVA, t-tests, effect sizes  
âœ… **Rich Visualizations** - 11 automated chart types  
âœ… **Interactive Demo** - Real-time Streamlit web app  
âœ… **Comprehensive Analysis** - Detailed statistical reports  
âœ… **Reproducible** - Caching system for consistency  
âœ… **Extensible** - Easy to add scenarios, cultures, models  
âœ… **Well-Documented** - 8,000+ lines of documentation  
âœ… **Production Ready** - Error handling, logging, validation

---

## ðŸ”¬ Research Methodology

Based on peer-reviewed research:
1. **Tao et al. (2024)** - Cultural prompting methodology
2. **Naous et al. (2024)** - Cultural bias measurement
3. **Hofstede (2011)** - Cultural dimensions framework
4. **Zheng et al. (2024)** - LLM-as-judge evaluation

**Key Innovation**: Automated baseline bias detection to reveal learned cultural preferences.

---

## ðŸ“Š Experimental Results (Summary)

### Key Findings

1. **Inherent India Bias**: All models closest to Indian values in baseline (distance: 1.066)
2. **Model Performance**: DeepSeek best overall (6.63), GPT-4o-mini best stereotypes (9.83)
3. **Collectivist Advantage**: 19% better alignment for collectivist vs individualistic cultures
4. **Decision Patterns**: US shows highest diversity (entropy: 0.856), Mexico lowest (0.720)
5. **Value Priorities**: Collectivist cultures prioritize "Duty", US prioritizes "Individual Freedom"
6. **Scenario Difficulty**: Family scenarios hardest (6.51), Resource easiest (6.72)
7. **Statistical Significance**: Cultures differ significantly (p<0.001), models don't (p=0.96)

### Dataset

- **1,440 responses** (20 scenarios Ã— 4 models Ã— 6 cultures Ã— 3 runs)
- **100% parse success rate**
- **Date**: November 17, 2025

---

## ðŸš€ Getting Started

**For New Users**:
1. Read `QUICKSTART.md` (5 minutes)
2. Run quick test (2 minutes)
3. Explore demo app

**For Researchers**:
1. Read `README.md` (15 minutes)
2. Review `BASELINE_TESTING.md` methodology
3. Run full experiment (45 minutes)
4. Analyze statistical report

**For Developers**:
1. Read this `PROJECT_SUMMARY.md`
2. Review core system files
3. Understand data flow
4. Extend as needed

---

## ðŸ¤ Contribution Areas

1. **New Scenarios**: Domain-specific situations (business, medical, legal)
2. **New Cultures**: Africa, South America, Eastern Europe
3. **New Models**: Local LLMs, fine-tuned models
4. **New Metrics**: Alternative evaluation approaches
5. **Multilingual**: Native language testing
6. **Optimizations**: Speed, cost, accuracy improvements

---

## ðŸ“ž Support

- **Setup Issues**: Check `QUICKSTART.md` troubleshooting
- **Methodology Questions**: Read `BASELINE_TESTING.md`
- **Code Questions**: Review this file + inline docstrings
- **Bugs**: Run `python test.py` for diagnostics

---

## ðŸ“„ License

MIT License - Open for research and commercial use

---

## ðŸ‘¤ Author

**Kabin Wang**  
WorldWise AI - Cultural Bias Research Lab

---

## ðŸŽ¯ Quick Reference

| Task | Command | Time | Cost |
|------|---------|------|------|
| Setup | `pip install -r requirements.txt` | 2 min | Free |
| Verify | `python test.py` | 30 sec | Free |
| Quick Test | `python main.py --mode quick` | 2 min | $0.10 |
| Visualize | `python visualizer.py results/*.csv` | 30 sec | Free |
| Analyze | `python analyze.py results/*.csv` | 30 sec | Free |
| Demo | `streamlit run demo.py` | Instant | Free |
| Full Run | `python main.py --mode full` | 45 min | $20-40 |

---

**Last Updated**: November 17, 2025  
**Version**: 2.0  
**Status**: Production Ready âœ…
