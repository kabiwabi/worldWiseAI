# Cultural Bias Measurement in Large Language Models

Automated framework for measuring cultural bias in LLMs through role-playing prompts and evaluation across 20 culturally-ambiguous scenarios.

## ğŸ¯ Core Features

- **Baseline Testing** - Measures inherent cultural bias without cultural context
- **Multi-Model Support** - GPT-4o-mini, Claude 3.5 Haiku, Gemini 2.5 Flash Lite, DeepSeek
- **6 Cultural Contexts** - Baseline, US, Japan, India, Mexico, UAE (based on Hofstede dimensions)
- **20 Scenarios** - Family, Career, Social, Resource Allocation categories
- **4 Automated Metrics** - Cultural alignment, consistency, differentiation, stereotype detection
- **Interactive Demo** - Streamlit web app for real-time exploration
- **8 Visualization Types** - Automated chart generation including baseline bias analysis

## ğŸ“‚ Project Structure

```
cultural_llm_bias/
â”œâ”€â”€ Core System (6 files)
â”‚   â”œâ”€â”€ config.py              # Configuration & Hofstede scores
â”‚   â”œâ”€â”€ scenarios.py           # 20 culturally-ambiguous scenarios
â”‚   â”œâ”€â”€ prompt_constructor.py  # Cultural role-playing prompts
â”‚   â”œâ”€â”€ llm_interface.py       # Multi-provider API interface
â”‚   â”œâ”€â”€ response_parser.py     # Response extraction
â”‚   â””â”€â”€ evaluator.py           # Automated metrics
â”œâ”€â”€ Execution (5 files)
â”‚   â”œâ”€â”€ main.py                # Experiment runner
â”‚   â”œâ”€â”€ demo.py                # Interactive Streamlit app
â”‚   â”œâ”€â”€ visualizer.py          # Chart generation (8 types)
â”‚   â”œâ”€â”€ analyze.py             # Statistical analysis
â”‚   â””â”€â”€ test.py                # System verification
â””â”€â”€ Results
    â”œâ”€â”€ results_TIMESTAMP.csv  # Raw data
    â”œâ”€â”€ results_TIMESTAMP.json # Structured results
    â”œâ”€â”€ summary_TIMESTAMP.json # Summary + baseline bias
    â”œâ”€â”€ analysis_report_*.txt  # Statistical analysis
    â””â”€â”€ visualizations/        # 8 generated plots
```

## ğŸš€ Quick Start

### 1. Install & Setup
```bash
pip install -r requirements.txt
export OPENAI_API_KEY="..."
export ANTHROPIC_API_KEY="..."
export GOOGLE_API_KEY="..."
export DEEPSEEK_API_KEY="..."  # Optional
python test.py  # Verify installation
```

### 2. Run Quick Test (2 scenarios)
```bash
python main.py --mode quick --scenarios 2
```

### 3. Generate Visualizations
```bash
python visualizer.py results/results_*.csv
```

### 4. View Analysis
```bash
python analyze.py results/results_*.csv
```

### 5. Launch Interactive Demo
```bash
streamlit run demo.py
```

## ğŸ“Š Latest Results (Nov 2024)

**Full Experiment**: 1,440 responses (20 scenarios Ã— 4 models Ã— 6 cultures Ã— 3 runs)

### Model Performance

| Model | Alignment | Std Dev | Stereotype Avoidance |
|-------|-----------|---------|---------------------|
| DeepSeek | **6.62** | 1.28 | 9.42 |
| Claude 3.5 Haiku | 6.60 | 1.33 | 7.33 |
| Gemini 2.5 Flash | 6.57 | 1.17 | 8.50 |
| GPT-4o-mini | 6.57 | 1.23 | **9.75** |

**Finding**: DeepSeek achieves best overall balance (highest alignment + strong stereotype avoidance) at lowest cost.

### Cultural Alignment by Culture

- **India**: 8.02/10 â­ï¸ (highest)
- **Japan**: 6.86/10
- **UAE**: 6.44/10
- **US**: 5.83/10
- **Mexico**: 5.80/10

**Insight**: Models excel at collectivist cultures (India) vs. individualistic ones (US).

## ğŸ¯ Methodology

### Baseline Testing
Tests without cultural context to reveal inherent bias:
```python
Baseline: "You are a helpful assistant..."  # No cultural context
â†’ Reveals which culture model naturally aligns with
```

### Cultural Prompting
Role-playing with specific cultural identities:
```python
"You are a 28-year-old professional in Tokyo, Japan,
born and raised in Japan with typical Japanese cultural values."
â†’ Tests ability to adapt to different cultures
```

### Automated Metrics
- **Cultural Alignment** (0-10): Euclidean distance on Hofstede dimensions
- **Consistency** (0-10): Similar responses to similar scenarios
- **Differentiation** (0-10): Response variation across cultures
- **Stereotype Score** (0-10): Overuse of stereotypical language

## ğŸ“ˆ Output Files

### Experiment Results
- `results_TIMESTAMP.csv` - Raw data (all responses + metrics)
- `results_TIMESTAMP.json` - Structured JSON version
- `summary_TIMESTAMP.json` - Statistics + baseline bias analysis
- `experiment.log` - Execution log

### Generated Visualizations
- `cultural_alignment_by_model.png` - Bar chart comparison
- `differentiation_heatmap.png` - Culture Ã— Model heatmap
- `decision_distribution.png` - Decision breakdowns
- `value_frequency.png` - Most common values
- `stereotype_scores.png` - Stereotype detection
- `model_comparison_radar.png` - Multi-metric comparison
- `category_performance.png` - Performance by scenario type
- `baseline_comparison.png` - Baseline bias analysis

## ğŸ¤– Supported Models

| Model | Provider | Cost | Notes |
|-------|----------|------|-------|
| GPT-4o-mini | OpenAI | $ | Best stereotype avoidance |
| Claude 3.5 Haiku | Anthropic | $$ | Fast, consensus-seeking |
| Gemini 2.5 Flash | Google | $ | Latest, balanced |
| DeepSeek | DeepSeek | $ | Best overall, cost-effective |

## ğŸ“š Documentation

- **README.md** (this file) - Overview & quick start
- **QUICKSTART.md** - 5-minute setup guide
- **BASELINE_TESTING.md** - Detailed baseline methodology
- **PROJECT_SUMMARY.md** - Complete file index

## ğŸ”¬ Research Foundation

Based on:
- **Tao et al. (2024)** - Cultural prompting methodology
- **Naous et al. (2024)** - Cultural bias measurement
- **Hofstede (2011)** - Cultural dimensions framework

## ğŸ’¡ Key Statistics

- **Total Code**: ~2,500 lines (14 Python files)
- **Documentation**: ~1,500 lines (4 guides)
- **Test Coverage**: 6 categories (100% core functionality)
- **Parse Success**: 100% (structured response format)
- **API Calls (full)**: 1,080 (~$20-40 depending on models)
- **API Calls (quick)**: 18 (2 scenarios)

## ğŸ› ï¸ Customization

```python
# Add new scenarios (scenarios.py)
Scenario(
    id="NEW001",
    category="Custom",
    text="Your scenario...",
    cultural_dimensions=["individualism", "power_distance"]
)

# Add new cultures (config.py)
CULTURAL_CONTEXTS["Brazil"] = CulturalContext(
    hofstede_scores={
        "individualism": 38,
        "power_distance": 69,
        # ... other dimensions
    }
)

# Add new models (config.py)
MODELS["new-model"] = ModelConfig(
    provider="openai",
    model_name="gpt-4",
    api_key_env="OPENAI_API_KEY"
)
```

## ğŸ¤ Usage Examples

### Full Experiment
```bash
python main.py --mode full  # All 20 scenarios, all models
```

### Custom Experiment
```bash
python main.py --mode quick --scenarios 5  # 5 scenarios, quick mode
```

### Analysis Pipeline
```bash
python main.py --mode quick --scenarios 2
python visualizer.py results/results_*.csv
python analyze.py results/results_*.csv
```

## âš™ï¸ Requirements

- Python 3.8+
- Dependencies: numpy, pandas, openai, anthropic, google-generativeai, matplotlib, seaborn, plotly, streamlit, tqdm
- API keys for chosen providers

## ğŸ“ Citation

If you use this framework, please cite the foundational research:
- Tao et al. (2024) - Cultural Prompting
- Naous et al. (2024) - Cultural Bias Measurement
- Hofstede (2011) - Cultural Dimensions Theory
